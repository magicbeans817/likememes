{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook downloads data about globally top selling games from digital gaming platform Steam. These data contain title, release date and information about reviews and prices of individual games. Output of this Downloader is CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download a few packages necessary for our downloader to be able to scrape data from Steam webpage, then other packages help us display raw data and pandas help us to create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by creating a class, which we call 'Downloader'. Functions of few first attributes of this downloader come naturally from their names. Then we create attributes 'dataf', 'hoarder' and 'download_data'. As the list of games is too long for just one webpage, Steam saved it in more than 600 webpages in total. Thus, downloading our data works in following way, using  'download_data' attribute:\n",
    "- It begins by creating a list of urls of given number of pages (starting from the first one) using 'hoarder' attribute.\n",
    "- Then it uses 'dataf' attribute, which downloads HTML for every url in the list, applies first few attributes of class 'Downloader' and creates pandas dataframe of data for every given url.\n",
    "- Last, 'download_data' appends dataframe for every url to one large dataframe we want to end up with. It also assigns indexes to individual games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of pages to scrape\n",
    "\n",
    "pages = 1\n",
    "\n",
    "class Downloader:\n",
    "    def __init__(self, link):\n",
    "        self.link = link\n",
    "        self.uClient = uReq(link)\n",
    "        self.page_html = self.uClient.read()\n",
    "        self.uClient.close()\n",
    "        self.soup = BeautifulSoup(self.page_html, \"lxml\")\n",
    "    \n",
    "    def get_titles(self):\n",
    "        td = self.soup.findAll('span', {\"class\":\"title\"})\n",
    "        titles = []\n",
    "        for ind in td:\n",
    "            # lstrip and rstrip remove symbols from sides, strip removes white spaces\n",
    "            titles.append(str(ind).lstrip('<span class=\"title\">').rstrip('span>').rstrip('</'))\n",
    "        return titles\n",
    "    \n",
    "    def get_release_dates(self):\n",
    "        td = self.soup.findAll('div', {\"class\":\"col search_released responsive_secondrow\"})\n",
    "        release_dates = []\n",
    "        for ind in td:\n",
    "            if ind.text != \"\":\n",
    "            # lstrip and rstrip remove symbols from sides, strip removes white spaces\n",
    "                release_dates.append(str(ind).rstrip('</div>').split(\">\")[-1])\n",
    "            else:\n",
    "                release_dates.append(None)\n",
    "        return release_dates\n",
    "    \n",
    "    def reviews(self):\n",
    "   \n",
    "        td = self.soup.findAll('span', {\"class\":\"search_review_summary mixed\"} or\n",
    "                               {\"class\":\"search_review_summary positive\"} or\n",
    "                              {\"class\":\"search_review_summary negative\"})\n",
    "        reviews = []\n",
    "        for ind in td:\n",
    "            # lstrip and rstrip remove symbols from sides, strip removes white spaces\n",
    "            reviews.append(str(ind).split(\"html=\")[-1])\n",
    "        return reviews\n",
    "     \n",
    "    def get_share_positive_reviews(self):\n",
    "        text = self.reviews()\n",
    "        shares = []\n",
    "        for percent in text:\n",
    "            shares.append(percent.split(\"%\")[0].split(\"br&gt;\")[1])\n",
    "        return shares\n",
    "        \n",
    "    def get_number_user_reviews(self):  \n",
    "        text = self.reviews()\n",
    "        numbers = []\n",
    "        for number in text:\n",
    "            start = number.find(\"of the \") + len(\"of the \")\n",
    "            end = number.find(\" user reviews\")\n",
    "            numbers.append(number[start:end].replace(\",\",\"\"))\n",
    "        return numbers\n",
    "        \n",
    "    def get_prices(self):\n",
    "\n",
    "        td = self.soup.findAll('div', {\"class\":\"col search_price_discount_combined responsive_secondrow\"} or \n",
    "                          {\"class\":\"col search_price discounted responsive_secondrow\"})\n",
    "        prices = []\n",
    "        for ind in td:\n",
    "            if \"888888\" not in str(ind):\n",
    "                if (len(str(ind).split(\"\\r\\n\")[-1].split(\"</div>\\n</div>\")[0].strip()) <10):\n",
    "                    prices.append(str(ind).split(\"\\r\\n\")[-1].split(\"</div>\\n</div>\")[0].strip().replace(\"€\",\"\").replace(\",\",\".\").replace(\"-\",\"0\").replace(\"Free\",\"0\"))\n",
    "            \n",
    "                else:\n",
    "                    prices.append(None)\n",
    "            else:\n",
    "                start1 = str(ind).find(\"><strike>\") + len(\"><strike>\")\n",
    "                end1 = str(ind).find(\"</strike>\")\n",
    "                prices.append(str(ind)[start1:end1].replace(\"€\",\"\").replace(\",\",\".\").replace(\"-\",\"0\").replace(\"Free\",\"0\"))\n",
    "        return prices\n",
    "    \n",
    "    def get_price_after_sale(self):\n",
    "\n",
    "        td = self.soup.findAll('div', {\"class\":\"col search_price_discount_combined responsive_secondrow\"} or \n",
    "                          {\"class\":\"col search_price discounted responsive_secondrow\"})\n",
    "        sales = []\n",
    "        for ind in td:\n",
    "            if \"888888\" not in str(ind):\n",
    "                sales.append(None)\n",
    "            #elif (len(str(ind).split(\"\\r\\n\")[-1].split(\"</div>\\n</div>\")[0].strip()) >10):\n",
    "                #sales.append(0)\n",
    "            else:\n",
    "                sales.append(str(ind).split(\"br/>\")[-1].split(\"€\")[0].replace(\",\",\".\").replace(\"-\",\"0\").replace(\"Free\",\"0\").strip())\n",
    "        return sales\n",
    "    \n",
    "    def get_rate_of_sale(self):\n",
    "\n",
    "        td = self.soup.findAll('div', {\"class\":\"col search_price_discount_combined responsive_secondrow\"} or \n",
    "                          {\"class\":\"col search_price discounted responsive_secondrow\"})\n",
    "        percent = []\n",
    "        for ind in td:\n",
    "            if \"888888\" not in str(ind):\n",
    "                percent.append(None)\n",
    "            else:\n",
    "                start = str(ind).find(\">\\n<span>-\")+len(\">\\n<span>-\")\n",
    "                end = str(ind).find(\"%\")\n",
    "                percent.append(str(ind)[start:end])\n",
    "        return percent\n",
    "    \n",
    "    def dataf(self):\n",
    "        titles = self.get_titles()\n",
    "        dates = self.get_release_dates()\n",
    "        share_reviews = self.get_share_positive_reviews()\n",
    "        number_reviews = self.get_number_user_reviews()\n",
    "        normal_prices = self.get_prices()\n",
    "        sale_price = self.get_price_after_sale()\n",
    "        sale_rate = self.get_rate_of_sale()\n",
    "        \n",
    "        \n",
    "        self.data = pd.DataFrame({\n",
    "             'Title': pd.Series(titles),\n",
    "             'Release date': pd.to_datetime(pd.Series(dates),format='%d %b, %Y', errors = 'coerce'),\n",
    "             'Share of positive reviews (in %)': pd.to_numeric(share_reviews, errors = 'coerce'),\n",
    "             'Total number of reviews': pd.to_numeric(number_reviews, errors = 'coerce'),\n",
    "             'Normal price (€)': pd.to_numeric(normal_prices, errors = 'coerce'),\n",
    "             'Discounted price if there is a sale (€)': pd.to_numeric(sale_price, errors = 'coerce'),\n",
    "             'Sale rate (in %)': pd.to_numeric(sale_rate, errors = 'raise')})\n",
    "        return self.data\n",
    "   \n",
    "    def hoarder(self):\n",
    "        urls = []\n",
    "        #last_page = int(self.soup.findAll('a',{'class':\"search_result_row ds_collapse_flag  app_impression_tracked\"})[-1].find('a')['href'][-2:])\n",
    "        for i in range(pages):\n",
    "            urls.append(self.link + f\"&page={451+i}\")\n",
    "        return urls\n",
    "    \n",
    "    def download_data(self):\n",
    "        urls = self.hoarder()\n",
    "        Frame = pd.DataFrame()\n",
    "        for url in urls:\n",
    "            print(url)\n",
    "            Frame = Frame.append(pd.DataFrame(data = Downloader(url).dataf()))\n",
    "        Frame.index = range(pages*25)\n",
    "        return Frame\n",
    "    \n",
    "    \n",
    "    \n",
    "        #451+i\n",
    "        #df = pd.DataFrame.from_dict(d, orient='index')\n",
    "        #df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the link for Steam page of global top sellers ordered by reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "25\n",
      "20\n",
      "20\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "link = 'https://store.steampowered.com/search/?sort_by=Reviews_DESC&os=win&filter=globaltopsellers'\n",
    "link2 = 'https://store.steampowered.com/search/?sort_by=Reviews_DESC&os=win&filter=globaltopsellers&page=451'\n",
    "second = Downloader(link2)\n",
    "uClient = uReq(link2)\n",
    "page_html = uClient.read()\n",
    "uClient.close()\n",
    "soup = BeautifulSoup(page_html, \"lxml\")\n",
    "print(len(second.get_rate_of_sale()))\n",
    "print(len(second.get_titles()))\n",
    "print(len(second.get_release_dates()))\n",
    "print(len(second.get_share_positive_reviews()))\n",
    "print(len(second.get_number_user_reviews()))\n",
    "print(len(second.get_prices()))\n",
    "print(len(second.get_price_after_sale()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize the link in order to continue working with it. Next, we can apply the 'download_data' attribute and take a look at the craped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://store.steampowered.com/search/?sort_by=Reviews_DESC&os=win&filter=globaltopsellers&page=451\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-5e788f9d98b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#first.download() # in order to explore page html in a reasonable way, one can use online javascript beautifier, available at:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# beautifier.io\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-3e3f65578c37>\u001b[0m in \u001b[0;36mdownload_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[0mFrame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[0mFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-3e3f65578c37>\u001b[0m in \u001b[0;36mdataf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m              \u001b[1;34m'Normal price (€)'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_prices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'coerce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m              \u001b[1;34m'Discounted price if there is a sale (€)'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msale_price\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'coerce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m              'Sale rate (in %)': pd.to_numeric(sale_rate, errors = 'raise')})\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    409\u001b[0m             )\n\u001b[0;32m    410\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         ]\n\u001b[1;32m--> 257\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "first = Downloader(link)\n",
    "#first.download() # in order to explore page html in a reasonable way, one can use online javascript beautifier, available at:\n",
    "# beautifier.io\n",
    "df = first.download_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the data as CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save the data as CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('steam_global_sellers_by_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
